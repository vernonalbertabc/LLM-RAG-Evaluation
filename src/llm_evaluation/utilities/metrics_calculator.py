"""
Metrics Calculator for LLM Evaluation
Contains utility functions for calculating various evaluation metrics.
"""

import os
import numpy as np
from typing import List, Dict, Any, Tuple
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI


from ragas import SingleTurnSample
from ragas.metrics import Faithfulness
from ragas import evaluate
from ragas.metrics import faithfulness
from datasets import Dataset
from ragas.llms import LangchainLLMWrapper
from langchain_openai import ChatOpenAI

class MetricsCalculator:
    """Utility class for calculating various evaluation metrics."""
    
    def __init__(self, openai_api_key: str = None, judge_model: str = "gpt-4.1"):
        """
        Initialize the MetricsCalculator.
        
        Args:
            openai_api_key: OpenAI API key for LLM-based calculations
            judge_model: Model to use for RAGAS evaluation (default: gpt-4o-mini)
        """
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.judge_model = judge_model
        self._sentence_model = None
        self._ragas_llm = None
    
    def _get_sentence_model(self) -> SentenceTransformer:
        """Get or initialize the sentence transformer model."""
        if self._sentence_model is None:
            self._sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        return self._sentence_model
    
    def _get_ragas_llm(self):
        """Get or initialize the RAGAS LLM wrapper."""
        if self._ragas_llm is None:
            chat_llm = ChatOpenAI(model=self.judge_model, api_key=self.openai_api_key)
            self._ragas_llm = LangchainLLMWrapper(chat_llm)
        return self._ragas_llm
    
    def calculate_precision_recall(self, retrieved_chunks: List[str], relevant_chunks: List[str]) -> Tuple[float, float]:
        """
        Calculate precision and recall for retrieval.
        
        Args:
            retrieved_chunks: Chunks returned by the retriever
            relevant_chunks: Ground truth chunks that should be retrieved
            
        Returns:
            Tuple of (precision, recall)
        """
        
        if not retrieved_chunks:
            return 0.0, 0.0
        
        if not relevant_chunks:
            return 0.0, 0.0
        
        found_relevant_chunks = set()
        for chunk in retrieved_chunks:
            for relevant_text in relevant_chunks:
                if relevant_text.lower() in chunk.lower():
                    found_relevant_chunks.add(relevant_text)
        
        relevant_retrieved = 0
        for chunk in retrieved_chunks:
            if any(relevant_text.lower() in chunk.lower() for relevant_text in relevant_chunks):
                relevant_retrieved += 1

        precision = relevant_retrieved / len(retrieved_chunks) if retrieved_chunks else 0.0
        
        recall = len(found_relevant_chunks) / len(relevant_chunks) if relevant_chunks else 0.0
        
        return precision, recall
    
    def calculate_ndcg(self, retrieved_chunks: List[str], relevant_chunks: List[str]) -> float:
        """
        Calculate Normalized Discounted Cumulative Gain (nDCG) for ranking evaluation.
        
        Args:
            retrieved_chunks: Chunks returned by the retriever (in order)
            relevant_chunks: Ground truth chunks that should be retrieved
            
        Returns:
            nDCG score (0 to 1, higher is better)
        """
        if not retrieved_chunks:
            return 0.0
        
        if not relevant_chunks:
            return 0.0
        
        relevance_scores = []
        for chunk in retrieved_chunks:
            is_relevant = any(relevant_text.lower() in chunk.lower() for relevant_text in relevant_chunks)
            relevance_scores.append(1.0 if is_relevant else 0.0)
        
        # Calculate DCG (Discounted Cumulative Gain)
        dcg = 0.0
        for i, score in enumerate(relevance_scores):
            if score > 0:
                dcg += score / (np.log2(i + 2))       

        ideal_scores = sorted(relevance_scores, reverse=True)
        idcg = 0.0
        for i, score in enumerate(ideal_scores):
            if score > 0:
                idcg += score / (np.log2(i + 2))
        
        ndcg = dcg / idcg if idcg > 0 else 0.0
        
        return ndcg
    
    def calculate_rouge_scores(self, generated_answer: str, ground_truth_answer: str) -> Dict[str, float]:
        """
        Calculate ROUGE-L scores for generated answer against ground truth.
        
        Args:
            generated_answer: Answer generated by the RAG chain
            ground_truth_answer: Expected ground truth answer
            
        Returns:
            Dictionary containing ROUGE-L F1, precision, and recall scores
        """

        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)            
        scores = scorer.score(ground_truth_answer, generated_answer)
            
        return {
                "rougeL_f1": scores['rougeL'].fmeasure,
                "rougeL_precision": scores['rougeL'].precision,
                "rougeL_recall": scores['rougeL'].recall
            }            

    
    def calculate_relevancy_score(self, question: str, generated_answer: str) -> float:
        """
        Calculate relevancy score between question and generated answer using sentence transformers.
        
        Args:
            question: User's original question
            generated_answer: Answer generated by the RAG chain
            
        Returns:
            Relevancy score between 0 and 1 (higher means more relevant)
        """

        model = self._get_sentence_model()       

        question_embedding = model.encode([question])
        answer_embedding = model.encode([generated_answer])
        similarity = cosine_similarity(question_embedding, answer_embedding)[0][0]            
        relevancy_score = (similarity + 1) / 2            
        return float(relevancy_score)
            
  
    def calculate_faithfulness_score_ragas(self, question: str, contexts: List[str], answer: str, ground_truths: List[str] = None) -> Dict[str, Any]:
        """
        Calculate faithfulness score using RAGAS with configurable judge model.
        
        This method follows the RAGAS approach as described in the guide:
        Faithfulness measures the factual accuracy of the generated response based on the retrieved documents.
        
        Args:
            question: The question asked
            contexts: List of retrieved context chunks
            answer: Answer generated by the RAG chain
            ground_truths: Optional list of ground truth answers (for compatibility)
            
        Returns:
            Dictionary containing faithfulness score and detailed analysis
        """

        print(f'Using {self.judge_model} as judge model for RAGAS evaluation')
        print('question: ', question)
        print('answer: ', answer)
        print('contexts: ', contexts)
        print('ground_truths: ', ground_truths)

        # Get the configured LLM for RAGAS
        ragas_llm = self._get_ragas_llm()
        
        # Create faithfulness metric with custom LLM
        faithfulness_metric = Faithfulness(llm=ragas_llm)

        data_samples = {
            'question': [question], 
            'answer': [answer], 
            'contexts': [contexts],
            'ground_truths': [ground_truths]
        }
        dataset = Dataset.from_dict(data_samples)
        results = evaluate(dataset, metrics=[faithfulness_metric])
        print('RAGAS results: ', results)
        print('RAGAS results type: ', type(results))
        print('RAGAS results keys: ', results.keys() if isinstance(results, dict) else 'Not a dict')
        
        faithfulness_score = results['faithfulness'][0] if isinstance(results['faithfulness'], list) else results['faithfulness']
        
        return {
            "faithfulness_score": faithfulness_score,
            "question": question,
            "contexts": contexts,
            "answer": answer,
            "ground_truths": ground_truths,
            "method": f"ragas_with_{self.judge_model}",
            "judge_model": self.judge_model,
            "raw_results": str(results)
        }
    
    def calculate_overall_metrics(self, scores: List[float]) -> Dict[str, float]:
        """
        Calculate overall statistics for a list of scores.
        
        Args:
            scores: List of numerical scores
            
        Returns:
            Dictionary containing mean and standard deviation
        """
        
        return {
            "mean": np.mean(scores),
            "std": np.std(scores)
        } 